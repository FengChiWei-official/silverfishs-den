# Catastrophic Forgetting in LLM Fine-Tuning

## 基本信息
- **概念**：灾难性遗忘（Catastrophic Forgetting）
- **相关论文**：
  - Overcoming Catastrophic Forgetting in Neural Networks (Kirkpatrick et al., 2017) - EWC
  - Fine-Tuning Language Models from Human Preferences (Ziegler et al., 2019) - KL Penalty

## 核心问题
当我们在特定任务（如医学问答、代码生成）上微调一个预训练的大语言模型（LLM）时，模型往往会“忘记”它在预训练阶段学到的通用知识（如常识、语法、其他领域的知识）。这种现象被称为灾难性遗忘。

## 表现形式
- **能力退化**：微调后的模型在通用基准测试（如 MMLU, GSM8K）上的分数显著下降。
- **重复/模式崩溃**：模型变得只会输出微调数据中的特定模式，失去多样性。
- **指令遵循能力丧失**：如果只在特定格式的数据上微调，模型可能无法理解其他格式的指令。

## 解决方案
1. **参数高效微调 (PEFT)**：
   - 使用 LoRA, Adapter 等方法，冻结大部分预训练参数，只更新少量参数。这能有效保留原模型的知识。
2. **混合训练 (Replay / Mixing)**：
   - 在微调数据中混入一部分预训练数据（Pre-training Data）或通用指令数据（General Instructions）。
   - 例如：在训练代码模型时，混入 10% 的通用文本数据。
3. **正则化 (Regularization)**：
   - **KL 散度约束**：在 RLHF 中，强制微调后的策略 $\pi$ 与原始策略 $\pi_{ref}$ 之间的 KL 散度不能太大（PPO 中的做法）。
   - **EWC (Elastic Weight Consolidation)**：惩罚对旧任务重要的参数的改变。
4. **多任务学习 (Multi-task Learning)**：
   - 同时在多个任务上进行微调，而不是按顺序微调。

## 实验结果
- 研究表明，LoRA 等 PEFT 方法在缓解灾难性遗忘方面通常优于全量微调。
- 简单的“数据混合”策略往往是最有效且最容易实现的手段。

## 创新点
- 认识到 LLM 的“通用性”是其核心价值，微调不应以牺牲通用性为代价。

## 缺点与局限
- 混合数据会增加训练成本和数据准备的复杂度。
- KL 约束过强会限制模型在新任务上的学习能力（Plasticity-Stability Dilemma）。

## 应用价值
- 在构建垂直领域大模型（Domain-specific LLM）时，必须考虑如何平衡“领域专业性”和“通用能力”。

## 相关笔记
- [[reading/reading-notes/PEFT/LoRA-低秩适配]]
- [[05-InstructGPT]]
- [[10-PEFT-参数高效微调框架]]

## 个人总结
灾难性遗忘是 LLM 持续学习（Continual Learning）中的最大挑战。目前的最佳实践通常是“PEFT + 数据混合”。不要丢掉你的预训练数据，它们是模型的“根”。
