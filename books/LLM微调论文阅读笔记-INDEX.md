# LLM 微调论文阅读笔记索引

**标签**：#paper #reading-notes #llm #fine-tuning

> 收集和整理 LLM 微调相关论文的阅读笔记，逐步完善

---

## 📌 笔记结构

```
books/
├── 论文仓库导航.md                    # 论文获取渠道和资源链接
├── LLM微调论文阅读笔记-INDEX.md      # 本文件，总导航
└── reading-notes/                     # 各论文的详细阅读笔记
    ├── 01-LoRA-低秩适配.md
    ├── 02-QLoRA-量化LoRA.md
    ├── 03-OFT-正交微调.md
    ├── 04-SFT-监督式微调.md
    ├── 05-PPO-近端策略优化.md
    ├── 06-DPO-直接偏好优化.md
    ├── 07-KTO-卡尼曼特维斯基优化.md
    ├── 08-RLHF-强化学习人类反馈.md
    ├── 09-灾难性遗忘.md
    └── 10-PEFT-参数高效微调框架.md
```

---

## 📖 已完成笔记

### 第一类：参数高效微调 (PEFT)

| # | 论文 | 作者 | 发表 | 笔记 | 状态 |
|---|------|------|------|------|------|
| 01 | [[LoRA-低秩适配]] | Hu et al. | ICLR 2021 | [待整理] | ⏳ |
| 02 | [[QLoRA-量化LoRA]] | Dettmers et al. | ICLR 2023 | [待整理] | ⏳ |
| 03 | [[OFT-正交微调]] | Sun et al. | 2023 年 | [待整理] | ⏳ |
| 10 | [[PEFT-框架]] | HuggingFace | 2022 年 | [待整理] | ⏳ |

### 第二类：监督式微调 (SFT)

| # | 论文 | 作者 | 发表 | 笔记 | 状态 |
|---|------|------|------|------|------|
| 04 | [[SFT-监督式微调]] | - | - | [待整理] | ⏳ |
| 05 | [[InstructGPT]] | OpenAI | 2022 年 | [待整理] | ⏳ |
| 06 | [[Llama-2]] | Meta | 2023 年 | [待整理] | ⏳ |

### 第三类：对齐与强化学习

| # | 论文 | 作者 | 发表 | 笔记 | 状态 |
|---|------|------|------|------|------|
| 05 | [[PPO-近端策略优化]] | Schulman et al. | 2017 年 | [待整理] | ⏳ |
| 06 | [[DPO-直接偏好优化]] | Rafailov et al. | NeurIPS 2023 | [待整理] | ⏳ |
| 07 | [[KTO-卡尼曼特维斯基优化]] | Bering et al. | 2024 年 | [待整理] | ⏳ |
| 08 | [[RLHF-强化学习人类反馈]] | Christiano et al. | 2016 年 | [待整理] | ⏳ |

### 第四类：挑战与解决方案

| # | 论文 | 主题 | 笔记 | 状态 |
|---|------|------|------|------|
| 09 | [[灾难性遗忘]] | 持续学习 | [待整理] | ⏳ |
| - | [[量化方法综述]] | 模型压缩 | [待整理] | ⏳ |

---

## 🎯 阅读指南

### 推荐阅读顺序（初学者）

```
第一周：基础理论
├─ 01. LoRA（参数高效微调入门）
├─ 04. SFT（监督微调基础）
└─ 09. 灾难性遗忘（理解挑战）

第二周：进阶方法
├─ 02. QLoRA（实践优化）
├─ 03. OFT（稳定性改进）
└─ 10. PEFT 框架（全景总结）

第三周：对齐技术
├─ 08. RLHF（对齐基础）
├─ 05. PPO（工业标准）
└─ 06. DPO（新方向）

第四周：综合应用
├─ 07. KTO（理论创新）
└─ 实践项目：使用 LLaMA-Factory 实现微调
```

### 推荐阅读顺序（工程师）

```
第一周：快速入门
├─ 01. LoRA（参数高效方案）
├─ 02. QLoRA（实现细节）
└─ 实践：配置 LoRA 微调

第二周：生产优化
├─ 09. 灾难性遗忘（防范策略）
├─ 04. SFT（数据准备）
└─ 实践：构造训练数据集

第三周：对齐优化
├─ 06. DPO（成本效益最优）
├─ 05. PPO（工业标准）
└─ 实践：对齐微调实现

第四周：高级技巧
├─ 03. OFT（稳定性增强）
├─ 07. KTO（不平衡数据处理）
└─ 实践：性能优化和基准测试
```

---

## 📝 笔记模板

每篇论文的阅读笔记应包含：

```markdown
# 论文标题

## 基本信息
- 作者：
- 发表：
- 论文链接：
- 代码链接：

## 核心问题
论文要解决什么问题？

## 主要贡献
1. 贡献 1
2. 贡献 2
3. 贡献 3

## 技术要点
- 方法 1：...
- 方法 2：...

## 实验结果
- 数据集 1：...
- 数据集 2：...

## 创新点
与同类工作相比的创新之处

## 缺点与局限
论文存在的问题

## 应用价值
在实践中的应用前景

## 相关笔记
- [[相关论文 1]]
- [[相关论文 2]]

## 关键公式（如有）
$$公式$$

## 个人总结
自己的思考和见解
```

---

## 🔍 快速导航

### 按研究方向
- [[参数高效微调|PEFT 方向]] — LoRA、QLoRA、OFT
- [[对齐技术|对齐方向]] — PPO、DPO、KTO
- [[灾难性遗忘|持续学习]] — 遗忘防止、知识保护
- [[监督式微调|SFT]] — 数据准备、最佳实践

### 按应用场景
- 显存受限 → QLoRA、LoRA
- 多任务适配 → OFT、参数冻结
- 对齐模型 → DPO、PPO
- 防止遗忘 → PEFT、混合训练

### 按论文类型
- 原始论文 — 理论创新，方法论
- 工程应用 — 框架、工具、最佳实践
- 综述论文 — 全景总结、对比分析

---

## 📊 阅读进度

```
总计：10 篇核心论文
已完成：0 篇 (0%)
进行中：0 篇
待开始：10 篇 (100%)
```

---

## 💡 推荐阅读工具

### 本地笔记管理
- Obsidian（本仓库使用）
- Notion
- OneNote

### 论文管理工具
- Zotero — 文献管理和标注
- Mendeley — 论文同步和阅读
- ReadCube Papers — 学术论文阅读器

### 快速理解
- Connected Papers — 论文引用关系图
- Papers with Code — 论文代码关联
- Semantic Scholar — AI 驱动的推荐

---

## 🎓 学习资源补充

### 在线课程
- Andrew Ng 的 Machine Learning Specialization
- DeepLearning.AI 的 LLM 课程
- Fast.ai 的 Practical Deep Learning

### 书籍
- 《Deep Learning》(Goodfellow et al.)
- 《Attention is All You Need 及其后续》综述
- 《Reinforcement Learning: An Introduction》(Sutton & Barto)

### 博客与文章
- Hugging Face Blog
- OpenAI Research Blog
- DeepMind Blog

---

**标签**：#paper #index #llm #reading
**最后更新**：2025-11-23
**维护者**：@user
