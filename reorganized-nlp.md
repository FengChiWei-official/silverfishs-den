### **一、 考试题型分布**

- **单选题：** 20题 × 2分 = 40分
    
- **判断题：** 10题 × 1分 = 10分
    
- **填空题：** 10题 × 2分 = 20分
    
- **简答题：** 4题 × 4分 = 16分（预计）
    
- **综合题：** 1题 × 10分 = 10分
    

---

### **二、 核心考点整理**

#### **第 1 章：绪论**

1. **1.1 定义：** 了解自然语言处理（NLP）的基本定义及其在人工智能中的地位。
    
2. **1.2.2 & 1.2.3：** 重点看 NLP 的基本任务（如分词、POS等）和主要挑战（歧义性、多样性）。
    
3. **1.3 (1,2,3)：** NLP 的发展阶段（规则 -> 统计 -> 深度学习）。
    

#### **第 4 章：机器学习基础（重点）**

1. **机器学习三要素：** **模型**（确定假设空间）、**学习准则**（损失函数）、**优化算法**（如梯度下降）。
    
2. **损失函数 (Loss Function)：**
    
    - **MSE (均方误差)：** 常用于回归任务。
        
    - **0-1 Loss：** 直观但不连续、不可导，优化困难。
        
    - **交叉熵 (Cross Entropy)：** 常用于分类任务，与 Softmax 配合使用。
        
3. **梯度下降 (GD)：**
    
    - 类型：批量梯度下降 (BGD)、随机梯度下降 (SGD)、小批量梯度下降 (MBGD)。
        
4. **拟合问题：**
    
    - **欠拟合 vs 过拟合：** 过拟合解决方法（加正则、Dropout、**Early Stop**）。
        
5. **任务分类：** 回归与分类；监督学习、弱监督、强监督。
    
6. **模型演进：** Logistic 回归 
    
    ```
    →→
    ```
    
     Softmax（多分类）。
    
7. **神经网络组件：**
    
    - **激活函数：** Sigmoid, Tanh, ReLU（解决梯度消失）。
        
    - **FNN (前馈神经网络)：** 基本结构。
        
    - **计算图：** 自动微分的基础概念。
        
    - **CNN (卷积神经网络)：** 局部连接、权重共享。
        
8. **RNN (循环神经网络) - 重难点：**
    
    - **公式：** 
        
        ```
        ht=f(Wxt+Uht−1+b)ht​=f(Wxt​+Uht−1​+b)
        ```
        
        。掌握其循环结构。
        
    - **LSTM (长短期记忆网络)：** 掌握其通过“门控机制”（遗忘门、输入门、输出门）解决长程依赖和梯度消失的问题。
        
9. **Attention (注意力机制)：** 核心概念（计算 Query 和 Key 的相关性，对 Value 进行加权）。
    

#### **第 5 章：词表示 (Word Representation)**

1. **基本假设：** **分布假设**（Distributional Hypothesis）——上下文相似的词，其语义也相似。
    
2. **词袋模型 (BoW) 与 N-gram。**
    
3. **神经词嵌入 (Word Embedding)：**
    
    - 掌握其概念（低维、稠密、向量化）。
        
    - 了解 Word2Vec (CBOW/Skip-gram) 的基本思想。
        
4. **评价方法：**
    
    - **内部评价：** 词相似度、类比推理。
        
    - **外部评价：** 将词向量放入下游任务（如情感分析）看效果。
        

#### **第 6 章：语言模型 (Language Model)**

1. **基础概念：** 计算一个句子出现的概率 
    
    ```
    P(w1,w2,...,wn)P(w1​,w2​,...,wn​)
    ```
    
    。
    
2. **指标：** **PPL (Perplexity，困惑度)** —— PPL 越小，模型效果越好。
    
3. **平滑技术：** **+1 平滑 (Add-one/Laplace Smoothing)** 解决零概率问题。
    
4. **神经网络语言模型：**
    
    - **FNN LM：** 固定窗口大小。
        
    - **RNN LM：** 可以处理任意长度序列。
        
    - **参数量计算：** 重点关注 RNN 的参数计算（隐藏层到隐藏层 
        
        ```
        WhhWhh​
        ```
        
        ，输入到隐藏层 
        
        ```
        WxhWxh​
        ```
        
        ，偏置等）。
        

#### **第 7 章：中文分词**

1. **三大困难：** 分词规范、歧义识别（交叉歧义、组合歧义）、未登录词识别（OOV）。
    
2. **分词方法：** 规则方法（正向/逆向最大匹配）、统计方法（HMM, CRF）、深度学习方法。
    
3. **训练标注：** **BIES 标签体系** (Begin, Inside, End, Single)。
    

#### **第 8 章：句法分析 (Parsing)**

1. **CFG (上下文无关语法)：** 基本概念、组成部分（终结符、非终结符、产生式）。
    
2. **PCFG (概率上下文无关语法)：** 给每个产生式赋予概率。
    
3. **CKY 算法：** 基于动态规划的解码算法。
    
4. **基于转移的算法 (Transition-based)：** 掌握基本状态转换（Shift, Reduce）的概念（P178）。
    

#### **第 10 章：语义角色标注 (SRL)**

1. **基本概念：** 识别“谁”对“谁”做了“什么”（谓词-论元结构）。
    

#### **第 12 章：机器翻译/其他**

1. **基本概念：** 统计机器翻译 (SMT) 与 神经机器翻译 (NMT) 的区别，编码器-解码器 (Encoder-Decoder) 框架。
    

---

### **三、 复习建议与考前突击**

1. **综合题预测：**
    
    - **RNN 参数计算：** 可能会给出一个特定的 RNN 结构，要求计算总参数量。
        
    - **CKY 算法：** 手动演示一个小句子的解析过程。
        
    - **分词标注：** 给一句话，根据 BIES 标注法进行人工标注。
        
2. **简答题必背：**
    
    - 过拟合的解决方法有哪些？
        
    - 中文分词的难点是什么？
        
    - RNN 为什么会出现梯度消失？LSTM 是如何改进的？
        
    - 激活函数的作用是什么？
        
3. **公式重点：**
    
    - RNN 的状态更新公式。
        
    - Softmax 公式。
        
    - 交叉熵损失函数公式。
        

**注意：** Chap 2, 9, 11, 13, 15 以及 5.2 节已经明确不考，复习时直接跳过以节省时间。